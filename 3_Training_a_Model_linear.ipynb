{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plagiarism Detection Model\n",
    "\n",
    "Now that I've created training and test data, I'm ready to define and train a model. The goal is to train a binary classification model that learns to label an answer file as either plagiarized or not, based on the features.\n",
    "\n",
    "This task will be broken down into a few discrete steps:\n",
    "\n",
    "* Upload the data to S3.\n",
    "* Define a binary classification model and a training script.\n",
    "* Train the model and deploy it.\n",
    "* Evaluate the deployed classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# create an S3 bucket\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the training data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = 'plagiarism_data'\n",
    "\n",
    "# set prefix, a descriptive name for a directory  \n",
    "prefix = 'plagiarism_detector'\n",
    "\n",
    "# upload all data to S3\n",
    "input_data = sagemaker_session.upload_data(path = data_dir, bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# confirm that data is in S3 bucket\n",
    "empty_check = []\n",
    "for obj in boto3.resource('s3').Bucket(bucket).objects.all():\n",
    "    empty_check.append(obj.key)\n",
    "    print(obj.key)\n",
    "\n",
    "assert len(empty_check) !=0, 'S3 bucket is empty.'\n",
    "print('Test passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Modeling\n",
    "\n",
    "Now that I've uploaded the training data, it's time to define and train a model!\n",
    "\n",
    "The type of model can be:\n",
    "* Use a built-in classification algorithm, like LinearLearner.\n",
    "* Define a custom Scikit-learn classifier, a comparison of models can be found [here](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html).\n",
    "* Define a custom PyTorch neural network classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Create an Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import LinearLearner\n",
    "from sagemaker import LinearLearner\n",
    "\n",
    "# specify an output path\n",
    "prefix = 'plagiarism_detector'\n",
    "output_path = 's3://{}/{}'.format(bucket, prefix)\n",
    "\n",
    "# instantiate LinearLearner\n",
    "linear = LinearLearner(role=role,\n",
    "                       train_instance_count=1, \n",
    "                       train_instance_type='ml.c4.xlarge',\n",
    "                       predictor_type='binary_classifier',\n",
    "                       output_path=output_path,\n",
    "                       sagemaker_session=sagemaker_session,\n",
    "                       epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(data_dir, 'train.csv'), header=None)\n",
    "\n",
    "train_x = train.loc[:,0]\n",
    "train_y= train.loc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert features/labels to numpy\n",
    "train_x_np = train_x.astype('float32')\n",
    "train_y_np = train_y.astype('float32')\n",
    "\n",
    "# create RecordSet\n",
    "formatted_train_data = linear.record_set(train_x_np, labels=train_y_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear.fit(formatted_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# deploy the model to create a predictor\n",
    "linear_predictor = linear.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Evaluating The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# read in test data, assuming it is stored locally\n",
    "test_data = pd.read_csv(os.path.join(data_dir, \"test.csv\"), header=None, names=None)\n",
    "\n",
    "# labels are in the first column\n",
    "test_y = test_data.iloc[:,0]\n",
    "test_x = test_data.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test one prediction\n",
    "test_x_np = np.array(test_x).astype('float32')\n",
    "test_y = np.array(test_y)\n",
    "\n",
    "result = linear_predictor.predict(test_x_np[0])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First: generate predicted, class labels\n",
    "\n",
    "predictions = linear_predictor.predict(test_x_np)\n",
    "test_preds = np.array([prediction.label['predicted_label'].float32_tensor.values[0] for prediction in predictions])\n",
    "\n",
    "# test that your model generates the correct number of labels\n",
    "assert len(test_y_preds)==len(test_y), 'Unexpected number of predictions.'\n",
    "print('Test passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Second: calculate the test accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_y, test_y_preds)\n",
    "\n",
    "print(accuracy)\n",
    "\n",
    "## print out the array of predicted and true labels, if you want\n",
    "print('\\nPredicted class labels: ')\n",
    "print(test_y_preds)\n",
    "print('\\nTrue class labels: ')\n",
    "print(test_y.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Clean up Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# estimator.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# deleting bucket\n",
    "\n",
    "# bucket_to_delete = boto3.resource('s3').Bucket(bucket)\n",
    "# bucket_to_delete.objects.all().delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Further Directions\n",
    "\n",
    "* Train a classifier to predict the *category* (1-3) of plagiarism and not just plagiarized (1) or not (0).\n",
    "* Utilize a different and larger dataset to see if this model can be extended to other types of plagiarism.\n",
    "* Use language or character-level analysis to find different (and more) similarity features.\n",
    "* Write a complete pipeline function that accepts a source text and submitted text file, and classifies the submitted text as plagiarized or not.\n",
    "* Use API Gateway and a lambda function to deploy your model to a web application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
